# -*- coding: utf-8 -*-
import streamlit as st
import os
import random
import tempfile
import networkx as nx
from pyvis.network import Network
from pages.lightrag.lightrag import LightRAG, QueryParam
from pages.lightrag.llm.openai import openai_complete_if_cache, openai_embed
from pages.lightrag.utils import EmbeddingFunc
from pages.Functions.ExtractFileContents import extract_text
from pages.Functions.Constants import EMBEDDING_MODEL_MAPPING, HIGHSPEED_MODEL_MAPPING


async def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs) -> str:
    return await openai_complete_if_cache(
        st.session_state.llm_model,
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.environ.get('ZhiZz_API_KEY'),
        base_url=os.environ.get('ZhiZz_URL'),
        **kwargs,
    )


async def embedding_func(texts: list[str]):
    return await openai_embed(
        texts,
        model=st.session_state.embedding_model,
        api_key=os.environ.get('SiliconFlow_API_KEY'),
        base_url=os.environ.get('SiliconFlow_URL'),
    )


def get_user_working_dir(filename=None):
    base_dir = "user_workspaces"
    os.makedirs(base_dir, exist_ok=True)

    if not filename:
        raise ValueError("å¿…é¡»æä¾›æ–‡ä»¶åæ¥åˆ›å»ºå·¥ä½œç›®å½•")

    dirname = os.path.splitext(filename)[0]
    return os.path.join(base_dir, dirname)


def init_rag(filename=None):
    if not filename:
        raise ValueError("å¿…é¡»æä¾›æ–‡ä»¶åæ¥åˆå§‹åŒ–RAG")

    working_dir = get_user_working_dir(filename)
    if os.path.exists(working_dir):
        st.info(f"æ£€æµ‹åˆ°å·²å­˜åœ¨çš„çŸ¥è¯†åº“ï¼š{os.path.basename(working_dir)}")
    else:
        os.makedirs(working_dir)
        st.success(f"åˆ›å»ºæ–°çš„çŸ¥è¯†åº“ï¼š{os.path.basename(working_dir)}")

    rag = LightRAG(
        working_dir=working_dir,
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=1024,
            max_token_size=8192,
            func=embedding_func),
        addon_params={"language": "Simplified Chinese"}
    )
    return rag


@st.cache_data
def load_knowledge_graph(graph_path, show_isolated=False):
    """åŠ è½½å¹¶ç¼“å­˜çŸ¥è¯†å›¾è°±æ•°æ®"""
    if not os.path.exists(graph_path):
        return None

    try:
        G = nx.read_graphml(graph_path)
        if not show_isolated:
            G.remove_nodes_from(list(nx.isolates(G)))

        net = Network(height="100vh", notebook=True, directed=False)
        net.from_nx(G)
        for node in net.nodes:
            node["color"] = "#{:06x}".format(random.randint(0, 0xFFFFFF))
            if "description" in node:
                node["title"] = node["description"]

        for edge in net.edges:
            if "description" in edge:
                edge["title"] = edge["description"]
            edge["width"] = 2

        temp_dir = tempfile.gettempdir()
        temp_path = os.path.join(temp_dir, f'graph_{random.randint(0, 999999)}.html')
        net.show(temp_path)
        html_content = _load_temp_graph(temp_path)

        return html_content
    except Exception as e:
        st.warning(f"çŸ¥è¯†å›¾è°±æ˜¾ç¤ºå¤±è´¥: {str(e)}")
        return None


def _load_temp_graph(temp_path):
    with open(temp_path, 'r', encoding='utf-8') as f:
        html_content = f.read()

    custom_js = """
    <script>
    // ç­‰å¾…ç½‘ç»œå›¾åŠ è½½å®Œæˆ
    network.on("click", function(event) {
        if (event.nodes.length > 0) {
            var clickedNode = event.nodes[0];
            var allNodes = network.body.data.nodes.getIds();
            var connectedNodes = network.getConnectedNodes(clickedNode);

            // éšè—æ‰€æœ‰èŠ‚ç‚¹
            allNodes.forEach(function(nodeId) {
                network.body.data.nodes.update({
                    id: nodeId,
                    hidden: true
                });
            });

            // æ˜¾ç¤ºç‚¹å‡»çš„èŠ‚ç‚¹åŠå…¶ç›¸è¿çš„èŠ‚ç‚¹
            network.body.data.nodes.update({
                id: clickedNode,
                hidden: false
            });
            connectedNodes.forEach(function(nodeId) {
                network.body.data.nodes.update({
                    id: nodeId,
                    hidden: false
                });
            });

            // æ›´æ–°ç½‘ç»œå›¾
            network.redraw();
        } else {
            // ç‚¹å‡»ç©ºç™½åŒºåŸŸæ—¶æ˜¾ç¤ºæ‰€æœ‰èŠ‚ç‚¹
            var allNodes = network.body.data.nodes.getIds();
            allNodes.forEach(function(nodeId) {
                network.body.data.nodes.update({
                    id: nodeId,
                    hidden: false
                });
            });
            network.redraw();
        }
    });
    </script>
    """
    return html_content.replace('</body>', custom_js + '</body>')


@st.cache_data
def display_knowledge_graph(working_dir, show_isolated=False):
    """æ˜¾ç¤ºçŸ¥è¯†å›¾è°±"""
    graph_path = os.path.join(working_dir, 'graph_chunk_entity_relation.graphml')
    html_content = load_knowledge_graph(graph_path, show_isolated)

    if html_content:
        with st.expander("çŸ¥è¯†å›¾è°±è¯´æ˜"):
            st.info("ğŸ‘‡ è¿™æ˜¯ä»æ–‡æ¡£ä¸­æå–çš„çŸ¥è¯†å›¾è°±ï¼Œå±•ç¤ºäº†æ–‡æ¡£ä¸­çš„å®ä½“åŠå…¶å…³ç³»ã€‚æ‚¨å¯ä»¥ï¼š\n"
                    "- æ‹–åŠ¨èŠ‚ç‚¹è°ƒæ•´å¸ƒå±€\n"
                    "- æ»šè½®ç¼©æ”¾å›¾è°±\n"
                    "- é¼ æ ‡æ‚¬åœåœ¨èŠ‚ç‚¹ä¸ŠæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯\n"
                    "- ç‚¹å‡»èŠ‚ç‚¹æŸ¥çœ‹èŠ‚ç‚¹å­å›¾\n"
                    "- ç‚¹å‡»ç©ºç™½éƒ¨åˆ†æ˜¾ç¤ºå…¨éƒ¨èŠ‚ç‚¹\n")
        st.components.v1.html(html_content, height=800)
    else:
        st.info("æš‚æ— çŸ¥å›¾è°±æ•°æ®")


def _process_uploaded_file(uploaded_file):
    with st.spinner('æ­£åœ¨å¤„ç†æ–‡æ¡£(é¦–æ¬¡å¤„ç†æ–‡æ¡£ä¼šæ¯”è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…å“¦)...'):
        current_file = st.session_state.get('current_file', None)
        if current_file != uploaded_file.name:
            st.session_state.current_file = uploaded_file.name
            st.session_state.rag = init_rag(uploaded_file.name)
            st.session_state.has_document = False  # é‡ç½®æ–‡æ¡£çŠ¶æ€
            st.session_state.messages = []  # æ¸…ç©ºå†å²æ¶ˆæ¯

        if not st.session_state.has_document:
            content = extract_text(uploaded_file)
            if content:
                st.session_state.rag.insert(content)
                st.success('æ–‡æ¡£å¤„ç†å®Œæˆï¼')
                st.session_state.has_document = True


def main():
    st.title("LightRAG - åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ")
    with st.expander("ä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
        ğŸŒŸ **å¼€å¯å¢å¼ºæ£€ç´¢æ–°æ—¶ä»£** ğŸŒŸ
        
        âœ… **æºé¡¹ç›®åœ°å€**ï¼šhttps://github.com/HKUDS/LightRAG

        ğŸ§© **ç³»ç»Ÿäº®ç‚¹**ï¼š
        
        âœ… åˆ©ç”¨AIæ„å»ºç»†ç²’åº¦çŸ¥è¯†å›¾è°±<br>
        âœ… åŸºäºå›¾è°±å†…å®¹è¿›è¡Œå¢å¼ºå¼æ£€ç´¢(RAG)<br>
        âœ… å›¾è°±å†…å®¹å®æ—¶å±•ç¤º<br>

        ğŸ› ï¸ **æ“ä½œæŒ‡å—**ï¼š
        1. ä¸Šä¼ ä¸šåŠ¡æ–‡æ¡£ï¼ˆåˆåŒ/æŠ¥å‘Š/æ‰‹å†Œç­‰ï¼‰
        2. é€‰æ‹©æ™ºèƒ½æŸ¥è¯¢æ¨¡å¼
        3. æ¢ç´¢åŠ¨æ€ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±
        4. è·å–åŸºäºä¸Šä¸‹æ–‡çš„ç²¾å‡†å›ç­”

        ğŸŒ **å›¾è°±äº¤äº’æŠ€å·§**ï¼š
        ğŸ–±ï¸ å³é”®æ‹–åŠ¨æ¢ç´¢å›¾è°±å…¨æ™¯<br>
        ğŸ” ç‚¹å‡»èŠ‚ç‚¹èšç„¦å…³è”ç½‘ç»œ<br>
        ğŸšï¸ æ»‘åŠ¨è°ƒèŠ‚ä¿¡æ¯å¯†åº¦é˜ˆå€¼
        """, unsafe_allow_html=True)

    with st.sidebar:
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ–‡æ¡£",
            type=['pdf', 'xlsx', 'xls', 'txt', 'doc', 'docx'],
            help="æ”¯æŒçš„æ ¼å¼ï¼šPDFã€Excelæ–‡ä»¶(xlsx/xls)ã€æ–‡æœ¬æ–‡ä»¶(txt), Documentæ–‡ä»¶(doc/docx)"
        )

        if uploaded_file is not None:
            _process_uploaded_file(uploaded_file)

        if 'has_document' not in st.session_state:
            st.session_state.has_document = False

        st.subheader("ğŸ¤– æ¨¡å‹è®¾ç½®")
        model_display = st.selectbox("é€‰æ‹©æ¨¡å‹",
                                     list(HIGHSPEED_MODEL_MAPPING.keys()),
                                     index=0,
                                     help="é€‰æ‹©æ¨¡å‹",
                                     key="main_model_select")

        emb_model_display = st.selectbox("é€‰æ‹©æ¨¡å‹",
                                         list(EMBEDDING_MODEL_MAPPING.keys()),
                                         index=0,
                                         help="é€‰æ‹©ç”¨äºæ–‡æœ¬å‘é‡åŒ–çš„æ¨¡å‹",
                                         key="embed_model_select")

        new_llm_model = HIGHSPEED_MODEL_MAPPING[model_display]
        new_embedding_model = EMBEDDING_MODEL_MAPPING[emb_model_display]

        if (new_llm_model != st.session_state.get("llm_model") or
                new_embedding_model != st.session_state.get("embedding_model")):
            st.session_state.llm_model = new_llm_model
            st.session_state.embedding_model = new_embedding_model
            if "rag" in st.session_state and st.session_state.has_document:
                st.session_state.rag = init_rag(st.session_state.current_file)
                st.info("æ›´æ”¹æ¨¡å‹æˆåŠŸï¼")
        st.subheader("ğŸ” æŸ¥è¯¢æ¨¡å¼")
        query_mode = st.selectbox(
            "é€‰æ‹©æŸ¥è¯¢æ¨¡å¼",
            options=["naive", "local", "global", "hybrid", "mix"],
            index=4,
            help="""- naive: æœ´ç´ æ¨¡å¼ï¼Œç›´æ¥åŒ¹é…æœ€ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µ
                    - local: å±€éƒ¨æ¨¡å¼ï¼ŒåŒ¹é…æœ€ç›¸ä¼¼çš„å®ä½“å…³ç³»åŠé‚»æ¥å®ä½“å…³ç³»
                    - global: å…¨å±€æ¨¡å¼ï¼ŒåŒ¹é…æœ€ç›¸ä¼¼çš„å®ä½“ä»¥åŠé—´æ¥å®ä½“å…³ç³»
                    - hybrid: localæ¨¡å¼ + globalæ¨¡å¼
                    - mix: naiveæ¨¡å¼ + hybridæ¨¡å¼""",
            key="query_mode_select"
        )

        st.subheader("âš™ï¸ é«˜çº§å‚æ•°è®¾ç½®")
        with st.expander("å±•å¼€é«˜çº§å‚æ•°è®¾ç½®"):
            only_need_context = st.toggle("ä»…è¿”å›ä¸Šä¸‹æ–‡", help="å¼€å¯åå°†åªè¿”å›æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä¸è¿›è¡ŒLLMæ€»ç»“")
            only_need_prompt = st.toggle("ä»…è¿”å›æç¤ºè¯", help="å¼€å¯åå°†åªè¿”å›ç”Ÿæˆçš„æç¤ºè¯ï¼Œä¸è¿›è¡ŒLLMå›ç­”")
            response_type = st.selectbox(
                "å›ç­”æ ¼å¼",
                options=["Multiple Paragraphs", "Single Paragraph", "Bullet Points"],
                help="é€‰æ‹©AIå›ç­”çš„æ ¼å¼æ ·å¼",
                key="response_type_select"
            )
            top_k = st.slider("æ£€ç´¢æ•°é‡(top_k)", min_value=10, max_value=100, value=60,
                              help="åœ¨localæ¨¡å¼ä¸‹è¡¨ç¤ºæ£€ç´¢çš„å®ä½“æ•°é‡ï¼Œåœ¨globalæ¨¡å¼ä¸‹è¡¨ç¤ºæ£€ç´¢çš„å…³ç³»æ•°é‡")
            max_token_for_text_unit = st.slider("æ–‡æœ¬å•å…ƒæœ€å¤§tokenæ•°", min_value=1000, max_value=8000, value=4000,
                                                help="åŸå§‹æ–‡æœ¬å—çš„æœ€å¤§tokenæ•°é‡")
            max_token_for_global_context = st.slider("å…¨å±€ä¸Šä¸‹æ–‡æœ€å¤§tokenæ•°", min_value=1000, max_value=8000,
                                                     value=4000, help="å…³ç³»æè¿°çš„æœ€å¤§tokenæ•°é‡")
            max_token_for_local_context = st.slider("å±€éƒ¨ä¸Šä¸‹æ–‡æœ€å¤§tokenæ•°", min_value=1000, max_value=8000, value=4000,
                                                    help="å®ä½“æè¿°çš„æœ€å¤§tokenæ•°é‡")
            show_isolated_nodes = st.toggle("æ˜¾ç¤ºå­¤ç«‹èŠ‚ç‚¹", value=False, help="å¼€å¯åå°†æ˜¾ç¤ºæ²¡æœ‰ä»»ä½•è¿æ¥çš„èŠ‚ç‚¹")
        st.subheader("ğŸ—‘ï¸ æ¸…é™¤å†å²")
        if st.button("æ¸…é™¤å¯¹è¯å†å²"):
            st.session_state.rag_messages = []
            st.success("å·²æ¸…é™¤å¯¹è¯å†å²")

    if not st.session_state.has_document:
        st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£åå†å¼€å§‹å¯¹è¯")
        return
    col1, col2 = st.columns([4, 3])
    with col1:
        st.subheader("ğŸ’¬ å¯¹è¯")
        if "rag_messages" not in st.session_state:
            st.session_state.rag_messages = []

        for message in st.session_state.rag_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if prompt := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜"):
            with st.chat_message("user"):
                st.markdown(prompt)

            st.session_state.rag_messages.append({"role": "user", "content": prompt})
            with st.chat_message("assistant"):
                try:
                    assistant_response = st.session_state.rag.query(
                        query=prompt,
                        param=QueryParam(
                            mode=query_mode,
                            only_need_context=only_need_context,
                            only_need_prompt=only_need_prompt,
                            response_type=response_type,
                            top_k=top_k,
                            max_token_for_text_unit=max_token_for_text_unit,
                            max_token_for_global_context=max_token_for_global_context,
                            max_token_for_local_context=max_token_for_local_context,
                        )
                    )
                    st.markdown(assistant_response)
                    st.session_state.rag_messages.append({"role": "assistant", "content": assistant_response})

                    if len(st.session_state.rag_messages) > 20:
                        st.session_state.rag_messages = st.session_state.rag_messages[-20:]

                except Exception as outer_e:
                    st.error(f"å¤„ç†è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(outer_e)}")

    with col2:
        st.subheader("ğŸ“Š çŸ¥è¯†å›¾è°±")
        if not st.session_state.has_document:
            st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£ä»¥ç”ŸæˆçŸ¥è¯†å›¾è°±")
        else:
            display_knowledge_graph(st.session_state.rag.working_dir, show_isolated_nodes)


if 'previous_page' not in st.session_state:
    st.session_state.previous_page = 'lightRAG'
current_page = 'lightRAG'
print(current_page)
if current_page != st.session_state.previous_page:
        st.session_state.clear()
        st.session_state.previous_page = current_page
main()
