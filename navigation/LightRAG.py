# -*- coding: utf-8 -*-
import streamlit as st
import random
import tempfile
import networkx as nx
from pyvis.network import Network
from pages.lightrag.llm.openai import openai_complete_if_cache
from pages.lightrag.llm.siliconcloud import siliconcloud_embedding
from pages.lightrag import LightRAG, QueryParam
from pages.lightrag.utils import EmbeddingFunc, logger, set_verbose_debug
from pages.lightrag.kg.shared_storage import initialize_pipeline_status
from pages.lightrag.rerank import custom_rerank
from pages.Functions.ExtractFileContents import extract_text
from pages.Functions.Constants import EMBEDDING_MODEL_MAPPING, HIGHSPEED_MODEL_MAPPING, EMBEDDING_DIM, \
    RERANKER_MODEL_MAPPING
import os
import asyncio
import logging
import logging.config

loop = asyncio.new_event_loop()
asyncio.set_event_loop(loop)

st.set_page_config(
    page_title="çŸ¥è¯†å›¾è°±æ£€ç´¢",
    layout="wide",
    initial_sidebar_state="expanded"
)


def initialize():
    if "llm_model" not in st.session_state:
        st.session_state.llm_model = 'deepseek-chat'
    if "embedding_model" not in st.session_state:
        st.session_state.embed_model = 'Qwen/Qwen3-Embedding-8B'
    if "reranker_model" not in st.session_state:
        st.session_state.embed_model = 'Qwen/Qwen3-Reranker-8B'
    if "embed_dim" not in st.session_state:
        st.session_state.embed_dim = 4096
    if "rag_messages" not in st.session_state:
        st.session_state.rag_messages = []
    if 'has_document' not in st.session_state:
        st.session_state.has_document = False
    if 'current_query_nodes' not in st.session_state:
        st.session_state.current_query_nodes = []
    if 'current_query_edges' not in st.session_state:
        st.session_state.current_query_edges = []


def configure_logging():
    """Configure logging for the application"""

    # Reset any existing handlers to ensure clean configuration
    for logger_name in ["uvicorn", "uvicorn.access", "uvicorn.error", "lightrag"]:
        logger_instance = logging.getLogger(logger_name)
        logger_instance.handlers = []
        logger_instance.filters = []

    # Get log directory path from environment variable or use current directory
    log_dir = os.getenv("LOG_DIR", os.getcwd())
    log_file_path = os.path.abspath(
        os.path.join(log_dir, "lightrag_compatible_demo.log")
    )

    print(f"\nLightRAG compatible demo log file: {log_file_path}\n")
    os.makedirs(os.path.dirname(log_dir), exist_ok=True)

    # Get log file max size and backup count from environment variables
    log_max_bytes = int(os.getenv("LOG_MAX_BYTES", 10485760))  # Default 10MB
    log_backup_count = int(os.getenv("LOG_BACKUP_COUNT", 5))  # Default 5 backups

    logging.config.dictConfig(
        {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "%(levelname)s: %(message)s",
                },
                "detailed": {
                    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                },
            },
            "handlers": {
                "console": {
                    "formatter": "default",
                    "class": "logging.StreamHandler",
                    "stream": "ext://sys.stderr",
                },
                "file": {
                    "formatter": "detailed",
                    "class": "logging.handlers.RotatingFileHandler",
                    "filename": log_file_path,
                    "maxBytes": log_max_bytes,
                    "backupCount": log_backup_count,
                    "encoding": "utf-8",
                },
            },
            "loggers": {
                "lightrag": {
                    "handlers": ["console", "file"],
                    "level": "INFO",
                    "propagate": False,
                },
            },
        }
    )

    # Set the logger level to INFO
    logger.setLevel(logging.INFO)
    # Enable verbose debug if needed
    set_verbose_debug(os.getenv("VERBOSE_DEBUG", "false").lower() == "true")


async def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
    llm_model = st.session_state.get('llm_model')
    return await openai_complete_if_cache(
        llm_model,
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.environ.get('ZhiZz_API_KEY'),
        base_url=os.environ.get('ZhiZz_URL'),
        **kwargs,
    )


async def my_rerank_func(query: str, documents: list, top_n: int = None, **kwargs):
    reranker_model = st.session_state.get('reranker_model')
    return await custom_rerank(
        query=query,
        documents=documents,
        model=reranker_model,
        base_url="https://api.siliconflow.cn/v1/rerank",
        api_key=os.environ.get('SiliconFlow_API_KEY'),
        top_n=top_n or 10,
        **kwargs,
    )


async def init_rag(filename=None):
    if not filename:
        raise ValueError("å¿…é¡»æä¾›æ–‡ä»¶åæ¥åˆå§‹åŒ–RAG")

    working_dir = get_user_working_dir(filename)
    if os.path.exists(working_dir):
        st.info(f"æ£€æµ‹åˆ°å·²å­˜åœ¨çš„çŸ¥è¯†åº“ï¼š{os.path.basename(working_dir)}")
    else:
        os.makedirs(working_dir)
        st.success(f"åˆ›å»ºæ–°çš„çŸ¥è¯†åº“ï¼š{os.path.basename(working_dir)}")

    rag = LightRAG(
        working_dir=working_dir,
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=st.session_state.embed_dim,
            max_token_size=8192,
            func=lambda texts: siliconcloud_embedding(
                texts,
                model=st.session_state.embed_model,
                max_token_size=8192,
                api_key=os.environ.get('SiliconFlow_API_KEY')
            ),
        ),
        rerank_model_func=my_rerank_func,
        addon_params={"language": "Simplified Chinese"},
    )
    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag


def get_user_working_dir(filename=None):
    base_dir = "user_workspaces"
    os.makedirs(base_dir, exist_ok=True)

    if not filename:
        raise ValueError("å¿…é¡»æä¾›æ–‡ä»¶åæ¥åˆ›å»ºå·¥ä½œç›®å½•")

    dirname = os.path.splitext(filename)[0]
    return os.path.join(base_dir, dirname)


def process_grpah(G):
    net = Network(height="100vh", notebook=True, directed=False)
    net.from_nx(G)
    for node in net.nodes:
        node["color"] = "#{:06x}".format(random.randint(0, 0xFFFFFF))
        if "description" in node:
            node["title"] = node["description"]

    for edge in net.edges:
        if "description" in edge:
            edge["title"] = edge["description"]
        edge["width"] = 2

    st.info(f"ğŸ“Š å›¾è°±ç»Ÿè®¡: {len(G.nodes)} ä¸ªèŠ‚ç‚¹, {len(G.edges)} ä¸ªå…³ç³»")
    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, f'graph_{random.randint(0, 999999)}.html')
    net.show(temp_path)
    with open(temp_path, 'r', encoding='utf-8') as f:
        html_content = f.read()
    custom_js = """
<script>
// ç­‰å¾…ç½‘ç»œå›¾åŠ è½½å®Œæˆ
network.on("click", function(event) {
    if (event.nodes.length > 0) {
        var clickedNode = event.nodes[0];
        var allNodes = network.body.data.nodes.getIds();
        var connectedNodes = network.getConnectedNodes(clickedNode);
        
        // åˆ›å»ºå¯è§èŠ‚ç‚¹é›†åˆï¼ˆåŒ…æ‹¬ç‚¹å‡»çš„èŠ‚ç‚¹å’Œç›¸è¿èŠ‚ç‚¹ï¼‰
        var visibleNodeSet = new Set([clickedNode, ...connectedNodes]);
        
        // æ‰¹é‡å‡†å¤‡æ›´æ–°æ•°æ®
        var updateData = [];
        allNodes.forEach(function(nodeId) {
            updateData.push({
                id: nodeId,
                hidden: !visibleNodeSet.has(nodeId)
            });
        });
        
        // ä¸€æ¬¡æ€§æ‰¹é‡æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹
        network.body.data.nodes.update(updateData);
        
        // åŒæ—¶æ›´æ–°ç›¸å…³çš„è¾¹
        var allEdges = network.body.data.edges.getIds();
        var edgeUpdateData = [];
        allEdges.forEach(function(edgeId) {
            var edge = network.body.data.edges.get(edgeId);
            var shouldShow = visibleNodeSet.has(edge.from) && visibleNodeSet.has(edge.to);
            edgeUpdateData.push({
                id: edgeId,
                hidden: !shouldShow
            });
        });
        network.body.data.edges.update(edgeUpdateData);
        
    } else {
        // ç‚¹å‡»ç©ºç™½åŒºåŸŸæ—¶æ˜¾ç¤ºæ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹
        var allNodes = network.body.data.nodes.getIds();
        var allEdges = network.body.data.edges.getIds();
        
        // æ‰¹é‡æ›´æ–°æ‰€æœ‰èŠ‚ç‚¹ä¸ºå¯è§
        var nodeUpdateData = allNodes.map(function(nodeId) {
            return { id: nodeId, hidden: false };
        });
        network.body.data.nodes.update(nodeUpdateData);
        
        // æ‰¹é‡æ›´æ–°æ‰€æœ‰è¾¹ä¸ºå¯è§
        var edgeUpdateData = allEdges.map(function(edgeId) {
            return { id: edgeId, hidden: false };
        });
        network.body.data.edges.update(edgeUpdateData);
    }
});
</script>
"""
    html_content = html_content.replace('</body>', custom_js + '</body>')
    try:
        os.remove(temp_path)
    except Exception as e:
        pass
    return html_content


def display_knowledge_graph(working_dir, show_isolated=False):
    """æ˜¾ç¤ºçŸ¥è¯†å›¾è°±"""
    graph_path = os.path.join(working_dir, 'graph_chunk_entity_relation.graphml')
    if not os.path.exists(graph_path):
        return None
    try:
        G = nx.read_graphml(graph_path)
        if not show_isolated:
            G.remove_nodes_from(list(nx.isolates(G)))
        html_content = process_grpah(G)

    except Exception as e:
        st.warning(f"çŸ¥è¯†å›¾è°±æ˜¾ç¤ºå¤±è´¥: {str(e)}")
        html_content = None

    if html_content:
        with st.expander("çŸ¥è¯†å›¾è°±è¯´æ˜"):
            st.info("ğŸ‘‡ è¿™æ˜¯ä»æ–‡æ¡£ä¸­æå–çš„çŸ¥è¯†å›¾è°±ï¼Œå±•ç¤ºäº†æ–‡æ¡£ä¸­çš„å®ä½“åŠå…¶å…³ç³»ã€‚æ‚¨å¯ä»¥ï¼š\n"
                    "- æ‹–åŠ¨èŠ‚ç‚¹è°ƒæ•´å¸ƒå±€\n"
                    "- æ»šè½®ç¼©æ”¾å›¾è°±\n"
                    "- é¼ æ ‡æ‚¬åœåœ¨èŠ‚ç‚¹ä¸ŠæŸ¥çœ‹è¯¦ç»†ä¿¡æ¯\n"
                    "- ç‚¹å‡»èŠ‚ç‚¹æŸ¥çœ‹èŠ‚ç‚¹å­å›¾\n"
                    "- ç‚¹å‡»ç©ºç™½éƒ¨åˆ†æ˜¾ç¤ºå…¨éƒ¨èŠ‚ç‚¹\n")
        st.components.v1.html(html_content, height=800)
    else:
        st.info("æš‚æ— çŸ¥å›¾è°±æ•°æ®")


def create_current_query_subgraph():
    """åˆ›å»ºå½“å‰æŸ¥è¯¢çš„å­å›¾è°±å¹¶å±•ç¤º"""
    st.subheader("ğŸ” å½“å‰æŸ¥è¯¢å›¾è°±")
    if not st.session_state.current_query_nodes and not st.session_state.current_query_edges:
        st.info("æš‚æ— æŸ¥è¯¢å›¾è°±æ•°æ®ï¼Œè¯·å…ˆè¿›è¡ŒæŸ¥è¯¢")
        return

    try:
        G = nx.Graph()
        if st.session_state.current_query_nodes:
            for node_data in st.session_state.current_query_nodes:
                G.add_node(
                    str(node_data["id"]),
                    label=node_data["entity"],
                    entity_type=node_data["type"],
                    description=node_data["description"],
                    created_at=node_data["created_at"],
                    file_path=node_data["file_path"]
                )

        if st.session_state.current_query_edges:
            for edge_data in st.session_state.current_query_edges:
                edge_id = str(edge_data["id"])
                entity1_id = str(edge_data["entity1"])
                entity2_id = str(edge_data["entity2"])

                G.add_edge(
                    entity1_id,
                    entity2_id,
                    id=edge_id,
                    description=edge_data["description"],
                    created_at=edge_data["created_at"],
                    file_path=edge_data["file_path"]
                )

        if len(G.nodes) == 0:
            st.info("å›¾è°±ä¸­æ²¡æœ‰èŠ‚ç‚¹")
            return

        html_content = process_grpah(G)
        st.components.v1.html(html_content, height=800)

    except Exception as e:
        st.error(f"åˆ›å»ºæŸ¥è¯¢å›¾è°±æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        st.exception(e)


async def _process_uploaded_file(uploaded_file):
    with st.spinner('æ­£åœ¨å¤„ç†æ–‡æ¡£(é¦–æ¬¡å¤„ç†æ–‡æ¡£ä¼šæ¯”è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…å“¦)...'):
        current_file = st.session_state.get('current_file', None)
        if current_file != uploaded_file.name:
            st.session_state.current_file = uploaded_file.name
            st.session_state.rag = await init_rag(uploaded_file.name)
            st.session_state.has_document = False
            st.session_state.messages = []

        if not st.session_state.has_document:
            content = await extract_text(uploaded_file)
            if content:
                try:
                    await st.session_state.rag.ainsert(content)
                    st.success('æ–‡æ¡£å¤„ç†å®Œæˆï¼')
                    st.session_state.has_document = True
                except Exception as e:
                    st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                    st.session_state.has_document = False


async def print_stream(stream, placeholder):
    content = ''
    async for chunk in stream:
        if chunk:
            content += chunk
            placeholder.markdown(content)
    return content


async def main():
    initialize()
    st.title("LightRAG - åŸºäºçŸ¥è¯†å›¾è°±çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ")
    with st.expander("ä½¿ç”¨è¯´æ˜", expanded=False):
        st.markdown("""
        ğŸŒŸ **å¼€å¯å¢å¼ºæ£€ç´¢æ–°æ—¶ä»£** ğŸŒŸ
        
        âœ… **æºé¡¹ç›®åœ°å€**ï¼šhttps://github.com/HKUDS/LightRAG

        ğŸ§© **ç³»ç»Ÿäº®ç‚¹**ï¼š
        âœ… å¼‚æ­¥æŠ½å–çŸ¥è¯†å›¾è°±ï¼Œè·å¾—æ›´é«˜é€Ÿçš„ä½“éªŒ<br>
        âœ… åˆ©ç”¨AIæ„å»ºç»†ç²’åº¦çŸ¥è¯†å›¾è°±<br>
        âœ… åŸºäºå›¾è°±å†…å®¹è¿›è¡Œå¢å¼ºå¼æ£€ç´¢(RAG)<br>
        âœ… å›¾è°±å†…å®¹å®æ—¶å±•ç¤º<br>

        ğŸ› ï¸ **æ“ä½œæŒ‡å—**ï¼š
        1. ä¸Šä¼ ä¸šåŠ¡æ–‡æ¡£ï¼ˆåˆåŒ/æŠ¥å‘Š/æ‰‹å†Œç­‰ï¼‰
        2. é€‰æ‹©æ™ºèƒ½æŸ¥è¯¢æ¨¡å¼
        3. æ¢ç´¢åŠ¨æ€ç”Ÿæˆçš„çŸ¥è¯†å›¾è°±
        4. è·å–åŸºäºä¸Šä¸‹æ–‡çš„ç²¾å‡†å›ç­”

        ğŸŒ **å›¾è°±äº¤äº’æŠ€å·§**ï¼š
        ğŸ–±ï¸ å³é”®æ‹–åŠ¨æ¢ç´¢å›¾è°±å…¨æ™¯<br>
        ğŸ” ç‚¹å‡»èŠ‚ç‚¹èšç„¦å…³è”ç½‘ç»œ<br>
        ğŸšï¸ æ»‘åŠ¨è°ƒèŠ‚ä¿¡æ¯å¯†åº¦é˜ˆå€¼
        """, unsafe_allow_html=True)

    with st.sidebar:
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ–‡æ¡£",
            type=['pdf', 'xlsx', 'xls', 'txt', 'doc', 'docx'],
            help="æ”¯æŒçš„æ ¼å¼ï¼šPDFã€Excelæ–‡ä»¶(xlsx/xls)ã€æ–‡æœ¬æ–‡ä»¶(txt), Documentæ–‡ä»¶(doc/docx)"
        )

        st.subheader("ğŸ¤– æ¨¡å‹è®¾ç½®")
        model_display = st.selectbox("é€‰æ‹©è¯­è¨€æ¨¡å‹",
                                     list(HIGHSPEED_MODEL_MAPPING.keys()),
                                     index=0,
                                     help="é€‰æ‹©æ¨¡å‹",
                                     key="main_model_select")

        emb_model_display = st.selectbox("é€‰æ‹©åµŒå…¥æ¨¡å‹",
                                         list(EMBEDDING_MODEL_MAPPING.keys()),
                                         index=0,
                                         help="é€‰æ‹©ç”¨äºæ–‡æœ¬å‘é‡åŒ–çš„æ¨¡å‹",
                                         key="embed_model_select")
        use_reranker = st.checkbox("æ˜¯å¦å¯ç”¨é‡æ’åºæ¨¡å‹", value=False)
        if use_reranker:
            rerank_model_display = st.selectbox("é€‰æ‹©é‡æ’åºæ¨¡å‹",
                                                list(RERANKER_MODEL_MAPPING.keys()),
                                                index=0,
                                                help="é€‰æ‹©ç”¨äºæ–‡æœ¬å‘é‡åŒ–çš„æ¨¡å‹",
                                                key="reranker_model_select")
            st.session_state.reranker_model = RERANKER_MODEL_MAPPING[rerank_model_display]

        st.session_state.llm_model = HIGHSPEED_MODEL_MAPPING[model_display]
        st.session_state.embed_model = EMBEDDING_MODEL_MAPPING[emb_model_display]
        st.session_state.embed_dim = EMBEDDING_DIM[emb_model_display]

        if uploaded_file is not None:
            await _process_uploaded_file(uploaded_file)

        st.subheader("ğŸ” æŸ¥è¯¢æ¨¡å¼")
        query_mode = st.selectbox(
            "é€‰æ‹©æŸ¥è¯¢æ¨¡å¼",
            options=["local", "global", "hybrid", "mix"],
            index=3,
            help="""- local: å±€éƒ¨æ¨¡å¼ï¼ŒåŒ¹é…æœ€ç›¸ä¼¼çš„å®ä½“å…³ç³»åŠé‚»æ¥å®ä½“å…³ç³»
                    - global: å…¨å±€æ¨¡å¼ï¼ŒåŒ¹é…æœ€ç›¸ä¼¼çš„å®ä½“ä»¥åŠé—´æ¥å®ä½“å…³ç³»
                    - hybrid: localæ¨¡å¼ + globalæ¨¡å¼
                    - mix: naiveæ¨¡å¼ + hybridæ¨¡å¼""",
            key="query_mode_select"
        )

        st.subheader("âš™ï¸ é«˜çº§å‚æ•°è®¾ç½®")
        with st.expander("å±•å¼€é«˜çº§å‚æ•°è®¾ç½®"):
            only_need_context = st.toggle("ä»…è¿”å›ä¸Šä¸‹æ–‡", help="å¼€å¯åå°†åªè¿”å›æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä¸è¿›è¡ŒLLMæ€»ç»“")
            only_need_prompt = st.toggle("ä»…è¿”å›æç¤ºè¯", help="å¼€å¯åå°†åªè¿”å›ç”Ÿæˆçš„æç¤ºè¯ï¼Œä¸è¿›è¡ŒLLMå›ç­”")
            response_type = st.selectbox(
                "å›ç­”æ ¼å¼",
                options=["Multiple Paragraphs", "Single Paragraph", "Bullet Points"],
                help="é€‰æ‹©AIå›ç­”çš„æ ¼å¼æ ·å¼",
                key="response_type_select"
            )
            top_k = st.slider("æ£€ç´¢æ•°é‡(top_k)", min_value=1, max_value=120, value=10,
                              help="åœ¨localæ¨¡å¼ä¸‹è¡¨ç¤ºæ£€ç´¢çš„å®ä½“æ•°é‡ï¼Œåœ¨globalæ¨¡å¼ä¸‹è¡¨ç¤ºæ£€ç´¢çš„å…³ç³»æ•°é‡")
            chunk_top_k = st.slider("æ–‡æ¡£å—æ£€ç´¢æ•°é‡", min_value=1, max_value=20, value=5,
                                    help="åœ¨é‡æ’åºæ¨¡å¼ä¸‹ï¼Œç»è¿‡é‡æ’åºåè¿”å›æ–‡æ¡£å—æ•°é‡")
            max_entity_tokens = st.slider("å®ä½“æè¿°æœ€å¤§tokenæ•°", min_value=5000, max_value=20000, value=8000,
                                                help="æè¿°å®ä½“çš„æœ€å¤§tokenæ•°é‡")
            max_relation_tokens = st.slider("å…³ç³»æè¿°æœ€å¤§tokenæ•°", min_value=5000, max_value=8000,
                                                     value=10000, help="æè¿°å…³ç³»çš„æœ€å¤§tokenæ•°é‡")
            max_total_tokens = st.slider("å…¨å±€æœ€å¤§tokenæ•°", min_value=10000, max_value=32768, value=30000,
                                                    help="æ€»ä½“ä¸Šä¸‹æ–‡æœ€å¤§tokenæ•°é‡")
            show_isolated_nodes = st.toggle("æ˜¾ç¤ºå­¤ç«‹èŠ‚ç‚¹", value=False, help="å¼€å¯åå°†æ˜¾ç¤ºæ²¡æœ‰ä»»ä½•è¿æ¥çš„èŠ‚ç‚¹")
        st.subheader("ğŸ—‘ï¸ æ¸…é™¤å†å²")
        if st.button("æ¸…é™¤å¯¹è¯å†å²"):
            st.session_state.rag_messages = []
            st.success("å·²æ¸…é™¤å¯¹è¯å†å²")

    if not st.session_state.has_document:
        st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£åå†å¼€å§‹å¯¹è¯")
        return
    tab1, tab2 = st.tabs(['å¯¹è¯', 'å›¾è°±å…¨è²Œ'])
    with tab1:
        col1, col2 = st.columns([4, 3])
        with col1:
            st.subheader("ğŸ’¬ å¯¹è¯")

            for message in st.session_state.rag_messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

            if prompt := st.chat_input("è¾“å…¥æ‚¨çš„é—®é¢˜"):
                with st.chat_message("user"):
                    st.markdown(prompt)
                st.session_state.current_query_nodes = []
                st.session_state.current_query_edges = []

                st.session_state.rag_messages.append({"role": "user", "content": prompt})
                with st.chat_message("assistant"):
                    try:
                        assistant_response = await st.session_state.rag.aquery(
                            query=prompt,
                            param=QueryParam(
                                mode=query_mode,
                                only_need_context=only_need_context,
                                only_need_prompt=only_need_prompt,
                                response_type=response_type,
                                top_k=top_k,
                                chunk_top_k=chunk_top_k,
                                max_entity_tokens=max_entity_tokens,
                                max_relation_tokens=max_relation_tokens,
                                max_total_tokens=max_total_tokens,
                                conversation_history=st.session_state.rag_messages,
                                enable_rerank=use_reranker,
                                stream=True
                            )
                        )
                        placeholder = st.empty()
                        content = await print_stream(assistant_response, placeholder)
                        st.session_state.rag_messages.append({"role": "assistant", "content": content})
                    except Exception as e:
                        st.error(e)
                    finally:
                        if st.session_state.rag:
                            await st.session_state.rag.finalize_storages()

        with col2:
            create_current_query_subgraph()
    with tab2:
        st.subheader("ğŸ“Š çŸ¥è¯†å›¾è°±")
        if not st.session_state.has_document:
            st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£ä»¥ç”ŸæˆçŸ¥è¯†å›¾è°±")
        else:
            display_knowledge_graph(st.session_state.rag.working_dir, show_isolated_nodes)


if 'previous_page' not in st.session_state:
    st.session_state.previous_page = 'lightRAG'
current_page = 'lightRAG'
if current_page != st.session_state.previous_page:
    st.session_state.clear()
    st.session_state.previous_page = current_page
configure_logging()
asyncio.run(main())
