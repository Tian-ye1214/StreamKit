import streamlit as st
import hashlib
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from pages.Functions.ExtractFileContents import extract_text
from sklearn.metrics.pairwise import cosine_similarity
from pages.Functions.UserLogManager import KnowledgeBaseManager
from pages.Functions.Prompt import rag_prompt
from openai import OpenAI
from pages.Functions.Constants import HIGHSPEED_MODEL_MAPPING
import os
import re

kb_manager = KnowledgeBaseManager()


class RAG:
    def __init__(self, embedding_model_path, rerank_model_path):
        self.embedding_model_path = embedding_model_path
        self.rerank_model_path = rerank_model_path
        self.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. å®šä¹‰ embedding å‡½æ•°
    def hf_embed(self, texts: list[str], tokenizer, embed_model) -> np.ndarray:
        encoded_texts = tokenizer(
            texts, return_tensors="pt", padding=True, truncation=True
        ).to(self.DEVICE)
        with torch.no_grad():
            outputs = embed_model(
                input_ids=encoded_texts["input_ids"],
                attention_mask=encoded_texts["attention_mask"],
            )
            embeddings = outputs.last_hidden_state.mean(dim=1)
        if embeddings.dtype == torch.bfloat16:
            return embeddings.detach().to(torch.float32).cpu().numpy()
        else:
            return embeddings.detach().cpu().numpy()

    # 2. æ–‡æœ¬åˆ‡å‰²å‡½æ•°
    def split_text(self, text, chunk_size=1024, special_chars=None, overlap=128):
        if special_chars:
            # å…ˆæŒ‰ç‰¹æ®Šå­—ç¬¦åˆ†å‰²
            segments = []
            current_pos = 0
            for char in special_chars:
                parts = text[current_pos:].split(char)
                for i, part in enumerate(parts[:-1]):
                    if part.strip():
                        segments.append(part.strip())
                    current_pos += len(part) + len(char)
            if parts[-1].strip():
                segments.append(parts[-1].strip())

            chunks = []
            for segment in segments:
                if len(segment) > chunk_size:
                    start = 0
                    while start < len(segment):
                        end = min(start + chunk_size, len(segment))
                        chunks.append(segment[start:end])
                        start = end
                        if overlap > 0 and start < len(segment):
                            start = max(0, start - overlap)
                else:
                    chunks.append(segment)
            return chunks
        else:
            chunks = []
            start = 0
            while start < len(text):
                end = min(start + chunk_size, len(text))
                chunks.append(text[start:end])
                start = end
                if overlap > 0 and start < len(text):
                    start = max(0, start - overlap)
            return chunks

    # 3. è®¡ç®—æ–‡ä»¶ hash
    def file_hash(self, file_bytes):
        return hashlib.md5(file_bytes).hexdigest()

    # 4. åŠ è½½æ¨¡å‹å‡½æ•°
    def load_model(self):
        tokenizer = AutoTokenizer.from_pretrained(self.embedding_model_path)
        embed_model = AutoModel.from_pretrained(self.embedding_model_path, torch_dtype=torch.bfloat16)
        embed_model.eval()
        embed_model.to(self.DEVICE)
        return tokenizer, embed_model

    # 5. åŠ è½½rerankæ¨¡å‹å‡½æ•°
    def load_rerank_model(self):
        tokenizer = AutoTokenizer.from_pretrained(self.rerank_model_path)
        model = AutoModelForSequenceClassification.from_pretrained(self.rerank_model_path, torch_dtype=torch.bfloat16)
        model.eval()
        model.to(self.DEVICE)
        return tokenizer, model

    # 6. ä½¿ç”¨rerankæ¨¡å‹é‡æ’åº
    def rerank_results(self, query, results, tokenizer, model, top_k):
        if not results:
            return []

        pairs = [[query, result["text"]] for result in results]

        # ä½¿ç”¨æ¨¡å‹è®¡ç®—åˆ†æ•°
        with torch.no_grad():
            inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(
                self.DEVICE)
            scores = model(**inputs, return_dict=True).logits.view(-1, ).float().cpu().numpy()

        for i, score in enumerate(scores):
            results[i]["rerank_score"] = float(score)
        results.sort(key=lambda x: x["rerank_score"], reverse=True)

        return results[:top_k]

    def retrieval(self, user_input, tokenizer, embed_model, knowledge_bases):
        query_embedding = self.hf_embed([user_input], tokenizer, embed_model)[0]

        # å­˜å‚¨æ‰€æœ‰ç›¸ä¼¼åº¦ç»“æœ
        all_results = []

        # éå†æ‰€æœ‰çŸ¥è¯†åº“æ–‡ä»¶
        for kb in knowledge_bases:
            data = kb_manager.get_knowledge_base(st.session_state.current_user, kb['file_id'])

            # è®¡ç®—ç›¸ä¼¼åº¦
            embeddings = np.array([item["embedding"] for item in data])
            similarities = cosine_similarity([query_embedding], embeddings)[0]

            # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨
            for i, similarity in enumerate(similarities):
                all_results.append({
                    "text": data[i]["text"],
                    "similarity": similarity,
                    "source": f"knowledge_{kb['file_id']}.json"
                })

        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è·å–top-k
        all_results.sort(key=lambda x: x["similarity"], reverse=True)
        top_results = all_results[:st.session_state.top_k]

        if st.session_state.use_rerank:
            with st.spinner("æ­£åœ¨ä½¿ç”¨Rerankæ¨¡å‹é‡æ’åº..."):
                rerank_tokenizer, rerank_model = self.load_rerank_model()
                candidate_results = all_results[:min(st.session_state.top_k * 4, len(all_results))]
                top_results = self.rerank_results(user_input, candidate_results, rerank_tokenizer,
                                                        rerank_model,
                                                        st.session_state.top_k)
        return top_results


def initialization(rag_system):
    if "chunk_size" not in st.session_state:
        st.session_state.chunk_size = 1024
    if "overlap" not in st.session_state:
        st.session_state.overlap = 128
    if "special_chars" not in st.session_state:
        st.session_state.special_chars = ""
    if "top_k" not in st.session_state:
        st.session_state.top_k = 5
    if "use_rerank" not in st.session_state:
        st.session_state.use_rerank = False
    if "Client" not in st.session_state:
        st.session_state.Client = OpenAI(api_key=os.environ.get('ZhiZz_API_KEY'), base_url=os.environ.get('ZhiZz_URL'))
    if 'rag_text' not in st.session_state:
        st.session_state.rag_text = []
    if 'current_user' not in st.session_state:
        st.session_state.current_user = None
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = "deepseek-chat"
    if 'embed_model' not in st.session_state:
        st.session_state.tokenizer, st.session_state.embed_model = rag_system.load_model()


def RAG_base(rag_system):
    col1, col2 = st.columns(2)

    if not st.session_state.current_user:
        st.info("è¯·å…ˆç™»å½•")

    with col1:
        st.header("æ–‡æ®µæ£€ç´¢")
        if st.session_state.current_user:
            knowledge_bases = kb_manager.list_knowledge_bases(st.session_state.current_user)

            if not knowledge_bases:
                st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“ï¼")

            if user_input := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key="rag_base_input"):
                top_results = rag_system.retrieval(user_input, st.session_state.tokenizer, st.session_state.embed_model, knowledge_bases)

                st.subheader(f"æœ€ç›¸ä¼¼çš„ {st.session_state.top_k} ä¸ªæ®µè½ï¼š")
                for i, result in enumerate(top_results, 1):
                    if st.session_state.use_rerank:
                        with st.expander(f"Rerankåˆ†æ•°: {result['rerank_score']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])
                    else:
                        with st.expander(f"ç›¸ä¼¼åº¦: {result['similarity']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])

    with col2:
        st.header("çŸ¥è¯†åº“æ„å»º")
        if st.session_state.current_user:
            uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£(æ”¯æŒå¤šä¸ªæ–‡ä»¶ä¸Šä¼ )", type=["pdf", "docx", "txt", "csv"], accept_multiple_files=True)
            if uploaded_files:
                
                for uploaded_file in uploaded_files:
                    st.write(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {uploaded_file.name}")
                    file_id = rag_system.file_hash(uploaded_file.read())
                    uploaded_file.seek(0)

                    existing_kb = kb_manager.get_knowledge_base(st.session_state.current_user, file_id)
                    if existing_kb:
                        st.success(f"æ–‡ä»¶ {uploaded_file.name} å·²å­˜åœ¨çŸ¥è¯†åº“ï¼Œæ— éœ€é‡å¤æ„å»ºã€‚")
                        st.write(f"å…± {len(existing_kb)} æ®µï¼Œå·²åŠ è½½ã€‚")
                    else:
                        text = extract_text(uploaded_file)
                        chunks = rag_system.split_text(text, chunk_size=st.session_state.chunk_size,
                                                       special_chars=st.session_state.special_chars,
                                                       overlap=st.session_state.overlap)
                        st.write(f"æ–‡æ¡£ {uploaded_file.name} è¢«åˆ‡å‰²ä¸º {len(chunks)} æ®µã€‚")

                        with st.spinner(f"æ­£åœ¨ä¸º {uploaded_file.name} ç”Ÿæˆ embedding..."):
                            embeddings = []
                            batch_size = 8
                            for i in range(0, len(chunks), batch_size):
                                batch = chunks[i:i + batch_size]
                                emb = rag_system.hf_embed(batch, st.session_state.tokenizer, st.session_state.embed_model)
                                embeddings.extend(emb.tolist())

                        data = [{"text": chunk, "embedding": emb} for chunk, emb in zip(chunks, embeddings)]
                        kb_manager.save_knowledge_base(st.session_state.current_user, file_id, data)
                        st.success(f"æ–‡ä»¶ {uploaded_file.name} çš„çŸ¥è¯†åº“å·²æ„å»ºï¼Œå…± {len(data)} æ®µã€‚")
                
                st.rerun()


def rag_with_ai(rag_system):
    if not st.session_state.current_user:
        st.info("è¯·å…ˆç™»å½•")
        return

    st.header("åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”")
    knowledge_bases = kb_manager.list_knowledge_bases(st.session_state.current_user)

    if not knowledge_bases:
        st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“ï¼")
        return

    if user_input := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key="rag_ai_input"):
        top_results = rag_system.retrieval(user_input, st.session_state.tokenizer, st.session_state.embed_model, knowledge_bases)

        context = "\n\n".join([f"å‚è€ƒå†…å®¹ {i + 1}:\n{result['text']}" for i, result in enumerate(top_results)])
        message = rag_prompt(user_input, context)

        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            try:
                reason_placeholder = st.empty()
                message_placeholder = st.empty()
                content = ""
                reasoning_content = ""

                for chunk in st.session_state.Client.chat.completions.create(
                        model=st.session_state.selected_model,
                        messages=message,
                        temperature=0.6,
                        max_tokens=8192,
                        stream=True
                ):
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if getattr(delta, 'reasoning_content', None):
                            reasoning_content += delta.reasoning_content
                            reason_placeholder.markdown(
                                f"<div style='background:#f0f0f0; border-radius:5px; padding:10px; margin-bottom:10px; font-size:14px;'>"
                                f"ğŸ¤” {reasoning_content}</div>",
                                unsafe_allow_html=True
                            )
                        if delta and delta.content is not None:
                            content += delta.content
                            message_placeholder.markdown(
                                f"<div style='font-size:16px; margin-top:10px;'>{content}</div>",
                                unsafe_allow_html=True
                            )

                # æ˜¾ç¤ºå‚è€ƒå†…å®¹
                st.markdown("### å‚è€ƒå†…å®¹")
                for i, result in enumerate(top_results, 1):
                    if st.session_state.use_rerank:
                        with st.expander(f"Rerankåˆ†æ•°: {result['rerank_score']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])
                    else:
                        with st.expander(f"ç›¸ä¼¼åº¦: {result['similarity']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])

            except Exception as e:
                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")


def knowledge_base_management():
    st.subheader("æˆ‘çš„çŸ¥è¯†åº“")
    if st.session_state.current_user is None:
        st.info("è¯·å…ˆç™»å½•")
    else:
        knowledge_bases = kb_manager.list_knowledge_bases(st.session_state.current_user)
        if not knowledge_bases:
            st.info("æ‚¨è¿˜æ²¡æœ‰ä¸Šä¼ ä»»ä½•çŸ¥è¯†åº“")
        else:
            if st.button("ä¸‹è½½æ‰€æœ‰çŸ¥è¯†åº“", key=f"DownloadAllKnowledgeBase"):
                zip_buffer = kb_manager.download_all_knowledge_bases(st.session_state.current_user)
                if zip_buffer:
                    st.download_button(
                        label="ç‚¹å‡»ä¸‹è½½æ‰€æœ‰çŸ¥è¯†åº“",
                        data=zip_buffer,
                        file_name=f"all_knowledge_bases_{st.session_state.current_user}.zip",
                        mime="application/zip",
                        key="download_all_btn"
                    )
                else:
                    st.error("ä¸‹è½½æ‰€æœ‰çŸ¥è¯†åº“å¤±è´¥")
            for kb in knowledge_bases:
                col1, col2, col3 = st.columns([2, 1, 2])
                with col1:
                    st.write(f"çŸ¥è¯†åº“: {kb['file_id']}")
                    st.write(f"åˆ›å»ºæ—¶é—´: {kb['created_time']}")
                    st.write(f"æ®µè½æ•°é‡: {kb['chunk_count']}æ®µ")
                
                with col2:
                    st.subheader("æ“ä½œ")
                    if st.button(f"ä¸‹è½½", key=f"download_{kb['file_id']}"):
                        zip_buffer = kb_manager.download_knowledge_base(st.session_state.current_user, kb['file_id'])
                        if zip_buffer:
                            st.download_button(
                                label=f"ç‚¹å‡»ä¸‹è½½",
                                data=zip_buffer,
                                file_name=f"knowledge_{kb['file_id']}.zip",
                                mime="application/zip",
                                key=f"download_btn_{kb['file_id']}"
                            )
                        else:
                            st.error(f"ä¸‹è½½çŸ¥è¯†åº“ {kb['file_id']} å¤±è´¥")

                    if st.button(f"åˆ é™¤", key=f"delete_{kb['file_id']}"):
                        if kb_manager.delete_knowledge_base(st.session_state.current_user, kb['file_id']):
                            st.success(f"çŸ¥è¯†åº“ {kb['file_id']} å·²åˆ é™¤")
                            st.rerun()
                        else:
                            st.error(f"åˆ é™¤çŸ¥è¯†åº“ {kb['file_id']} å¤±è´¥")

                with col3:
                    st.subheader("é¢„è§ˆ")
                    data = kb_manager.get_knowledge_base(st.session_state.current_user, kb['file_id'])
                    if data:
                        preview_text = data[0]["text"][:200] + "..." if len(data[0]["text"]) > 200 else data[0]["text"]
                        st.text_area("å†…å®¹é¢„è§ˆ", preview_text, height=150)
                    else:
                        st.info("æš‚æ— é¢„è§ˆå†…å®¹")


def main():
    rag_system = RAG(embedding_model_path='G:/ä»£ç /ModelWeight/bge-m3',
                     rerank_model_path='G:/ä»£ç /ModelWeight/bge-reranker-v2-m3')
    initialization(rag_system)
    with st.sidebar:
        st.header("ç”¨æˆ·ç®¡ç†")
        username = st.text_input("ç”¨æˆ·å", value="")
        user_col1, user_col2 = st.columns([1, 1])
        with user_col1:
            if st.button("ç™»å½•/æ³¨å†Œ", key="login"):
                if username.strip() == "":
                    st.error("ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
                else:
                    if not re.match("^[A-Za-z0-9\u4e00-\u9fff]+$", username):
                        st.error("ç”¨æˆ·ååªèƒ½åŒ…å«ä¸­æ–‡ã€å­—æ¯å’Œæ•°å­—")
                    else:
                        st.session_state.current_user = username
                        kb_manager.user_register(st.session_state.current_user)
                        st.success(f"æ¬¢è¿, {st.session_state.current_user}!")

        st.header("é…ç½®å‚æ•°")
        model_names = list(HIGHSPEED_MODEL_MAPPING.keys())
        selected_model_name = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=model_names,
            index=1
        )
        st.session_state.selected_model = HIGHSPEED_MODEL_MAPPING[selected_model_name]
        st.session_state.chunk_size = st.number_input("æ–‡æœ¬å—å¤§å°", min_value=64, max_value=8192, value=1024, step=64)
        st.session_state.overlap = st.number_input("é‡å å­—ç¬¦æ•°", min_value=0,
                                                   max_value=st.session_state.chunk_size // 2, value=128, step=64)
        st.session_state.special_chars = st.text_input("ç‰¹æ®Šåˆ†éš”ç¬¦ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰", value="")
        st.session_state.special_chars = [char.strip() for char in st.session_state.special_chars.split(
            ",")] if st.session_state.special_chars else None
        st.session_state.top_k = st.number_input("è¿”å›æœ€ç›¸ä¼¼çš„æ®µè½æ•°", min_value=1, max_value=20, value=5, step=1)
        st.session_state.use_rerank = st.checkbox("å¯ç”¨Reranké‡æ’åº", value=False)
        if st.session_state.use_rerank:
            st.info("å°†ä½¿ç”¨BAAI/bge-reranker-v2-m3æ¨¡å‹å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº")
    st.title("æ–‡æ¡£ä¸Šä¼ ä¸EmbeddingçŸ¥è¯†åº“æ„å»º")
    tab1, tab2, tab3 = st.tabs(['çŸ¥è¯†åº“ç®¡ç†', 'çŸ¥è¯†åº“æ„å»ºä¸å±•ç¤º', 'AIçŸ¥è¯†åº“é—®ç­”'])
    with tab1:
        knowledge_base_management()
    with tab2:
        RAG_base(rag_system)
    with tab3:
        rag_with_ai(rag_system)


if 'previous_page' not in st.session_state:
    st.session_state.previous_page = 'RAG'
current_page = 'RAG'
if current_page != st.session_state.previous_page:
    st.session_state.clear()
    st.session_state.previous_page = current_page
main()
