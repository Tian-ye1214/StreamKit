import json
import streamlit as st
import hashlib
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from pages.Functions.ExtractFileContents import extract_text
from sklearn.metrics.pairwise import cosine_similarity
from pages.Functions.UserLogManager import KnowledgeBaseManager
from pages.Functions.Prompt import rag_prompt, IntentRecognition
from openai import OpenAI
from pages.Functions.Constants import HIGHSPEED_MODEL_MAPPING
import os
import re
import asyncio

kb_manager = KnowledgeBaseManager()


class RAG:
    def __init__(self, embedding_model_path, rerank_model_path):
        self.embedding_model_path = embedding_model_path
        self.rerank_model_path = rerank_model_path
        self.DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        self.all_results = []

    def last_token_pool(self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """è·å–æ¯ä¸ªåºåˆ—æœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„embedding"""
        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
        if left_padding:
            return last_hidden_states[:, -1]
        else:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = last_hidden_states.shape[0]
            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]

    def format_instruction(self, instruction, query, doc, type='embedding'):
        if instruction is None:
            instruction = 'Given a search query, retrieve relevant passages that answer the query'
        if type == 'embedding':
            return f'Instruct: {instruction}\nQuery: {query}'
        else:
            output = "<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}".format(
                instruction=instruction, query=query, doc=doc)
        return output

    # 1. å®šä¹‰ embedding å‡½æ•°
    async def hf_embed(self, texts: list[str], tokenizer, embed_model) -> np.ndarray:
        texts_with_instruct = [self.format_instruction(None, text, None, 'embedding') for text in texts]
        encoded_texts = tokenizer(
            texts_with_instruct, return_tensors="pt", padding=True, truncation=True, max_length=8192
        ).to(self.DEVICE)
        with torch.no_grad():
            outputs = embed_model(
                input_ids=encoded_texts["input_ids"],
                attention_mask=encoded_texts["attention_mask"],
            )
            embeddings = self.last_token_pool(outputs.last_hidden_state, encoded_texts["attention_mask"])
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        if embeddings.dtype == torch.bfloat16:
            return embeddings.detach().to(torch.float32).cpu().numpy()
        else:
            return embeddings.detach().cpu().numpy()

    # 2. æ–‡æœ¬åˆ‡å‰²å‡½æ•°
    async def split_text(self, text, chunk_size=1024, special_chars=None, overlap=128):
        # å…ˆæŒ‰ç‰¹æ®Šå­—ç¬¦åˆ†å‰²
        if special_chars:
            segments = []
            current_pos = 0
            for char in special_chars:
                parts = text[current_pos:].split(char)
                for i, part in enumerate(parts[:-1]):
                    if part.strip():
                        segments.append(part.strip())
                    current_pos += len(part) + len(char)
            if parts[-1].strip():
                segments.append(parts[-1].strip())

            chunks = []
            for segment in segments:
                if len(segment) > chunk_size:
                    start = 0
                    while start < len(segment):
                        end = min(start + chunk_size, len(segment))
                        chunks.append(segment[start:end])
                        start = end
                        if overlap > 0 and start < len(segment):
                            start = max(0, start - overlap)
                else:
                    chunks.append(segment)
            return chunks
        else:
            chunks = []
            start = 0
            while start < len(text):
                end = min(start + chunk_size, len(text))
                chunks.append(text[start:end])
                start = end
                if overlap > 0 and start < len(text):
                    start = max(0, start - overlap)
            return chunks

    # 3. è®¡ç®—æ–‡ä»¶ hash
    async def file_hash(self, file_bytes):
        return hashlib.md5(file_bytes).hexdigest()

    # 4. åŠ è½½æ¨¡å‹å‡½æ•°
    async def load_embedding_model(self):
        tokenizer = AutoTokenizer.from_pretrained(self.embedding_model_path)
        embed_model = AutoModel.from_pretrained(self.embedding_model_path, torch_dtype=torch.bfloat16)
        embed_model.eval()
        embed_model.to(self.DEVICE)
        return tokenizer, embed_model

    # 5. åŠ è½½rerankæ¨¡å‹å‡½æ•°
    async def load_rerank_model(self):
        tokenizer = AutoTokenizer.from_pretrained(self.rerank_model_path, padding_side='left')
        model = AutoModelForCausalLM.from_pretrained(self.rerank_model_path, torch_dtype=torch.bfloat16)
        model.eval()
        model.to(self.DEVICE)
        # åˆå§‹åŒ–ç‰¹æ®Štoken
        self.token_false_id = tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = tokenizer.convert_tokens_to_ids("yes")

        self.prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        self.prefix_tokens = tokenizer.encode(self.prefix, add_special_tokens=False)
        self.suffix_tokens = tokenizer.encode(self.suffix, add_special_tokens=False)
        
        return tokenizer, model

    def process_inputs(self, pairs, tokenizer):
        inputs = tokenizer(
            pairs, padding=False, truncation='longest_first',
            return_attention_mask=False, max_length=8192 - len(self.prefix_tokens) - len(self.suffix_tokens)
        )
        for i, ele in enumerate(inputs['input_ids']):
            inputs['input_ids'][i] = self.prefix_tokens + ele + self.suffix_tokens
        inputs = tokenizer.pad(inputs, padding=True, return_tensors="pt", max_length=8192)
        for key in inputs:
            inputs[key] = inputs[key].to(self.DEVICE)
        return inputs

    @torch.no_grad()
    def compute_logits(self, inputs, model):
        batch_scores = model(**inputs).logits[:, -1, :]
        true_vector = batch_scores[:, self.token_true_id]
        false_vector = batch_scores[:, self.token_false_id]
        batch_scores = torch.stack([false_vector, true_vector], dim=1)
        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
        scores = batch_scores[:, 1].exp().tolist()
        return scores

    # 6. ä½¿ç”¨rerankæ¨¡å‹é‡æ’åº
    async def rerank_results(self, query, results, tokenizer, model, top_k):
        if not results:
            return []
        # å‡†å¤‡è¾“å…¥å¯¹
        pairs = [self.format_instruction(None, query, result["text"], type='rerank') for result in results]

        with torch.no_grad():
            inputs = self.process_inputs(pairs, tokenizer)
            scores = self.compute_logits(inputs, model)

        # æ›´æ–°ç»“æœåˆ†æ•°å¹¶æ’åº
        for i, score in enumerate(scores):
            results[i]["rerank_score"] = float(score)
        results.sort(key=lambda x: x["rerank_score"], reverse=True)

        return results[:top_k]

    async def retrieval_from_kb(self, kb, query_embedding, all_results):
        data = await kb_manager.get_knowledge_base(st.session_state.current_user, kb['file_id'])
        # è®¡ç®—ç›¸ä¼¼åº¦
        embeddings = np.array([item["embedding"] for item in data])
        similarities = cosine_similarity([query_embedding], embeddings)[0]

        # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨
        for i, similarity in enumerate(similarities):
            all_results.append({
                "text": data[i]["text"],
                "similarity": similarity,
                "source": f"knowledge_{kb['file_id']}.json"
            })

    async def retrieval(self, user_input, tokenizer, embed_model, knowledge_bases):
        embeddings = await self.hf_embed([user_input], tokenizer, embed_model)
        query_embedding = embeddings[0]

        tasks = [self.retrieval_from_kb(kb, query_embedding, self.all_results) for kb in knowledge_bases]
        await asyncio.gather(*tasks)

        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è·å–top-k
        self.all_results.sort(key=lambda x: x["similarity"], reverse=True)
        top_results = self.all_results[:st.session_state.top_k]

        if st.session_state.use_rerank:
            with st.spinner("æ­£åœ¨ä½¿ç”¨Rerankæ¨¡å‹é‡æ’åº..."):
                rerank_tokenizer, rerank_model = await self.load_rerank_model()
                candidate_results = self.all_results[:min(st.session_state.top_k * 3, len(self.all_results))]
                top_results = await self.rerank_results(user_input, candidate_results, rerank_tokenizer,
                                                        rerank_model,
                                                        st.session_state.top_k)
        return top_results


async def initialization(rag_system):
    if "chunk_size" not in st.session_state:
        st.session_state.chunk_size = 1024
    if "overlap" not in st.session_state:
        st.session_state.overlap = 128
    if "special_chars" not in st.session_state:
        st.session_state.special_chars = ""
    if "top_k" not in st.session_state:
        st.session_state.top_k = 2
    if "use_rerank" not in st.session_state:
        st.session_state.use_rerank = False
    if "Client" not in st.session_state:
        st.session_state.Client = OpenAI(api_key=os.environ.get('ZhiZz_API_KEY'), base_url=os.environ.get('ZhiZz_URL'))
    if 'rag_text' not in st.session_state:
        st.session_state.rag_text = []
    if 'current_user' not in st.session_state:
        st.session_state.current_user = None
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = "deepseek-chat"
    if 'embed_model' not in st.session_state:
        st.session_state.tokenizer, st.session_state.embed_model = await rag_system.load_embedding_model()


async def RAG_base(rag_system):
    col1, col2 = st.columns(2)

    if not st.session_state.current_user:
        st.info("è¯·å…ˆç™»å½•")

    with col1:
        st.header("æ–‡æ®µæ£€ç´¢")
        if st.session_state.current_user:
            knowledge_bases = await kb_manager.list_knowledge_bases(st.session_state.current_user)

            if not knowledge_bases:
                st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“ï¼")

            if user_input := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key="rag_base_input"):
                top_results = await rag_system.retrieval(user_input, st.session_state.tokenizer,
                                                         st.session_state.embed_model,
                                                         knowledge_bases)

                st.subheader(f"æœ€ç›¸ä¼¼çš„ {st.session_state.top_k} ä¸ªæ®µè½ï¼š")
                for i, result in enumerate(top_results, 1):
                    if st.session_state.use_rerank:
                        with st.expander(f"Rerankåˆ†æ•°: {result['rerank_score']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])
                    else:
                        with st.expander(f"ç›¸ä¼¼åº¦: {result['similarity']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])

    with col2:
        st.header("çŸ¥è¯†åº“æ„å»º")
        if st.session_state.current_user:
            uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡æ¡£(æ”¯æŒå¤šä¸ªæ–‡ä»¶ä¸Šä¼ )", type=["pdf", "docx", "txt", "csv"],
                                              accept_multiple_files=True)
            if uploaded_files:
                tasks = [processing_file(rag_system, file) for file in uploaded_files]
                await asyncio.gather(*tasks)


async def processing_file(rag_system, uploaded_file):
    st.write(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {uploaded_file.name}")
    file_id = await rag_system.file_hash(uploaded_file.read())

    existing_kb = await kb_manager.get_knowledge_base(st.session_state.current_user, file_id)
    if not existing_kb:
        text = await extract_text(uploaded_file)
        chunks = await rag_system.split_text(text, chunk_size=st.session_state.chunk_size,
                                             special_chars=st.session_state.special_chars,
                                             overlap=st.session_state.overlap)
        st.write(f"æ–‡æ¡£ {uploaded_file.name} è¢«åˆ‡å‰²ä¸º {len(chunks)} æ®µã€‚")

        with st.spinner(f"æ­£åœ¨ä¸º {uploaded_file.name} ç”Ÿæˆ embedding..."):
            embeddings = []
            batch_size = 8
            batch = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]
            task = [rag_system.hf_embed(text, st.session_state.tokenizer, st.session_state.embed_model) for text in
                    batch]
            result = await asyncio.gather(*task)
            for emb in result:
                embeddings.extend(emb.tolist())

        data = [{"text": chunk, "embedding": emb} for chunk, emb in zip(chunks, embeddings)]
        await kb_manager.save_knowledge_base(st.session_state.current_user, file_id, data)
        st.success(f"æ–‡ä»¶ {uploaded_file.name} çš„çŸ¥è¯†åº“å·²æ„å»ºï¼Œå…± {len(data)} æ®µã€‚")


async def rag_with_ai(rag_system):
    if not st.session_state.current_user:
        st.info("è¯·å…ˆç™»å½•")
        return

    st.header("åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”")
    knowledge_bases = await kb_manager.list_knowledge_bases(st.session_state.current_user)

    if not knowledge_bases:
        st.warning("è¯·å…ˆä¸Šä¼ æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“ï¼")
        return

    if user_input := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key="rag_ai_input"):
        top_results = []
        with st.spinner('æ„å›¾è¯†åˆ«ä¸­ï¼š'):
            Intentmessage = IntentRecognition(user_input)
            response = st.session_state.Client.chat.completions.create(
                model='deepseek-chat',
                messages=Intentmessage,
                temperature=0.6,
                max_tokens=8192,
            )
            st.markdown(f'è¯†åˆ«åˆ°æ„å›¾:{response.choices[0].message.content}')
            user_intents = json.loads(response.choices[0].message.content)
        for user_intent in user_intents.values():
            top_results.extend(await rag_system.retrieval(user_intent, st.session_state.tokenizer,
                                                          st.session_state.embed_model, knowledge_bases))
        context = "\n".join([f"å‚è€ƒå†…å®¹ {i + 1}:\n{result['text']}" for i, result in enumerate(top_results)])
        message = rag_prompt(user_input, context)

        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            try:
                reason_placeholder = st.empty()
                message_placeholder = st.empty()
                content = ""
                reasoning_content = ""

                for chunk in st.session_state.Client.chat.completions.create(
                        model=st.session_state.selected_model,
                        messages=message,
                        temperature=0.6,
                        max_tokens=8192,
                        stream=True
                ):
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if getattr(delta, 'reasoning_content', None):
                            reasoning_content += delta.reasoning_content
                            reason_placeholder.markdown(
                                f"<div style='background:#f0f0f0; border-radius:5px; padding:10px; margin-bottom:10px; font-size:14px;'>"
                                f"ğŸ¤” {reasoning_content}</div>",
                                unsafe_allow_html=True
                            )
                        if delta and delta.content is not None:
                            content += delta.content
                            message_placeholder.markdown(
                                f"<div style='font-size:16px; margin-top:10px;'>{content}</div>",
                                unsafe_allow_html=True
                            )

                # æ˜¾ç¤ºå‚è€ƒå†…å®¹
                st.markdown("### å‚è€ƒå†…å®¹")
                for i, result in enumerate(top_results, 1):
                    if st.session_state.use_rerank:
                        with st.expander(f"Rerankåˆ†æ•°: {result['rerank_score']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])
                    else:
                        with st.expander(f"ç›¸ä¼¼åº¦: {result['similarity']:.4f} - æ¥æº: {result['source']}"):
                            st.write(result["text"])

            except Exception as e:
                st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")


async def knowledge_base_management():
    st.subheader("æˆ‘çš„çŸ¥è¯†åº“")
    if st.session_state.current_user is None:
        st.info("è¯·å…ˆç™»å½•")
    else:
        knowledge_bases = await kb_manager.list_knowledge_bases(st.session_state.current_user)
        if not knowledge_bases:
            st.info("æ‚¨è¿˜æ²¡æœ‰ä¸Šä¼ ä»»ä½•çŸ¥è¯†åº“")
        else:
            if st.button("ä¸‹è½½æ‰€æœ‰çŸ¥è¯†åº“", key=f"DownloadAllKnowledgeBase"):
                zip_buffer = await kb_manager.download_all_knowledge_bases(st.session_state.current_user)
                if zip_buffer:
                    st.download_button(
                        label="ç‚¹å‡»ä¸‹è½½æ‰€æœ‰çŸ¥è¯†åº“",
                        data=zip_buffer.getvalue(),
                        file_name=f"all_knowledge_bases_{st.session_state.current_user}.zip",
                        mime="application/zip",
                        key="download_all_btn"
                    )
                else:
                    st.error("ä¸‹è½½æ‰€æœ‰çŸ¥è¯†åº“å¤±è´¥")
            for kb in knowledge_bases:
                col1, col2, col3 = st.columns([1, 2, 1])
                with col1:
                    st.write(f"çŸ¥è¯†åº“: {kb['file_id']}")
                    st.write(f"åˆ›å»ºæ—¶é—´: {kb['created_time']}")
                    st.write(f"æ®µè½æ•°é‡: {kb['chunk_count']}æ®µ")

                with col2:
                    st.subheader("é¢„è§ˆ")
                    data = await kb_manager.get_knowledge_base(st.session_state.current_user, kb['file_id'])
                    if data:
                        preview_text = data[0]["text"][:200] + "..." if len(data[0]["text"]) > 200 else data[0]["text"]
                        st.text_area("å†…å®¹é¢„è§ˆ", preview_text, height=150)
                    else:
                        st.info("æš‚æ— é¢„è§ˆå†…å®¹")

                with col3:
                    st.subheader("æ“ä½œ")
                    col_1, col_2 = st.columns(2)
                    with col_1:
                        if st.button(f"ä¸‹è½½", key=f"download_{kb['file_id']}"):
                            zip_buffer = await kb_manager.download_knowledge_base(st.session_state.current_user,
                                                                                  kb['file_id'])
                            if zip_buffer:
                                st.download_button(
                                    label=f"ç‚¹å‡»ä¸‹è½½",
                                    data=zip_buffer.getvalue(),
                                    file_name=f"knowledge_{kb['file_id']}.zip",
                                    mime="application/zip",
                                    key=f"download_btn_{kb['file_id']}"
                                )
                            else:
                                st.error(f"ä¸‹è½½çŸ¥è¯†åº“ {kb['file_id']} å¤±è´¥")

                    with col_2:
                        if st.button(f"åˆ é™¤", key=f"delete_{kb['file_id']}"):
                            if await kb_manager.delete_knowledge_base(st.session_state.current_user, kb['file_id']):
                                st.success(f"çŸ¥è¯†åº“ {kb['file_id']} å·²åˆ é™¤")
                                st.rerun()
                            else:
                                st.error(f"åˆ é™¤çŸ¥è¯†åº“ {kb['file_id']} å¤±è´¥")


async def main():
    st.markdown("""
    <h1 style='text-align: center;'>
        æ™ºè¯†å®åº“ - æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
    </h1>
    <div style='text-align: center; margin-bottom: 20px;'>
    </div>
    """, unsafe_allow_html=True)
    rag_system = RAG(embedding_model_path='G:/ä»£ç /ModelWeight/Qwen3-embedding',
                     rerank_model_path='G:/ä»£ç /ModelWeight/Qwen3-rerank')
    await initialization(rag_system)

    with st.expander("ğŸ“– é¡¹ç›®è¯´æ˜", expanded=False):
        st.markdown("""
        ğŸŒŸ **æ™ºè¯†å®åº“ - æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ** ğŸŒŸ
        
        ğŸ§© **ç³»ç»ŸåŠŸèƒ½**ï¼š
        âœ… æ™ºèƒ½æ–‡æ¡£é—®ç­”ï¼šåŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿ<br>
        âœ… çŸ¥è¯†åº“ç®¡ç†ï¼šæ”¯æŒæ–‡æ¡£ä¸Šä¼ ã€æ„å»ºå’Œç®¡ç†<br>
        âœ… å¤šæ ¼å¼æ”¯æŒï¼šæ”¯æŒPDFã€Wordã€TXTã€CSVç­‰æ ¼å¼<br>
        âœ… æ™ºèƒ½æ£€ç´¢ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ–‡æ¡£æ£€ç´¢<br>
        âœ… é‡æ’åºä¼˜åŒ–ï¼šæ”¯æŒRerankæ¨¡å‹ä¼˜åŒ–æ£€ç´¢ç»“æœ<br>
        âœ… å¤šæ¨¡å‹é€‰æ‹©ï¼šæ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹é€‰æ‹©<br>

        ğŸ“ **ä¸»è¦æ¨¡å—**ï¼š
        1. çŸ¥è¯†åº“ç®¡ç†
           - ä¸Šä¼ å’Œç®¡ç†ä¸ªäººçŸ¥è¯†åº“
           - æ”¯æŒæ–‡æ¡£é¢„è§ˆå’Œä¸‹è½½
           - çŸ¥è¯†åº“çš„å¢åˆ æ”¹æŸ¥æ“ä½œ

        2. çŸ¥è¯†åº“æ„å»ºä¸å±•ç¤º
           - æ–‡æ¡£ä¸Šä¼ å’Œæ–‡æœ¬æå–
           - æ™ºèƒ½æ–‡æœ¬åˆ†å‰²
           - æ–‡æ¡£å‘é‡åŒ–å­˜å‚¨
           - ç›¸ä¼¼åº¦æ£€ç´¢å±•ç¤º

        3. AIçŸ¥è¯†åº“é—®ç­”
           - åŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”
           - å¤šæ¨¡å‹æ”¯æŒ
           - å‚è€ƒå†…å®¹å±•ç¤º
           - æ¨ç†è¿‡ç¨‹å¯è§†åŒ–

        âš™ï¸ **æŠ€æœ¯ç‰¹ç‚¹**ï¼š
        - ä½¿ç”¨Qwen3-embeddingæ¨¡å‹è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
        - æ”¯æŒQwen3-rerankerè¿›è¡Œç»“æœé‡æ’åº
        - æ”¯æŒå¤šç”¨æˆ·çŸ¥è¯†åº“éš”ç¦»

        ğŸ’¡ **ä½¿ç”¨å»ºè®®**ï¼š
        - å»ºè®®å…ˆæ„å»ºçŸ¥è¯†åº“åå†è¿›è¡Œé—®ç­”
        - å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ–‡æœ¬å—å¤§å°å’Œé‡å åº¦
        - å¯ç”¨Rerankå¯ä»¥æå‡æ£€ç´¢è´¨é‡
        - é€‰æ‹©åˆé€‚çš„æ¨¡å‹ä»¥è·å¾—æœ€ä½³æ•ˆæœ

        âš ï¸ **æ³¨æ„äº‹é¡¹**ï¼š
        - è¯·ç¡®ä¿ä¸Šä¼ æ–‡æ¡£æ ¼å¼æ­£ç¡®
        - å»ºè®®æ§åˆ¶å•ä¸ªæ–‡æ¡£å¤§å°
        - æ³¨æ„ä¿æŠ¤ä¸ªäººéšç§ä¿¡æ¯
        - å®šæœŸå¤‡ä»½é‡è¦çŸ¥è¯†åº“
        """, unsafe_allow_html=True)

    with st.sidebar:
        st.header("ç”¨æˆ·ç®¡ç†")
        username = st.text_input("ç”¨æˆ·å", value="")
        user_col1, user_col2 = st.columns([1, 1])
        with user_col1:
            if st.button("ç™»å½•/æ³¨å†Œ", key="login"):
                if username.strip() == "":
                    st.error("ç”¨æˆ·åä¸èƒ½ä¸ºç©º")
                else:
                    if not re.match("^[A-Za-z0-9\u4e00-\u9fff]+$", username):
                        st.error("ç”¨æˆ·ååªèƒ½åŒ…å«ä¸­æ–‡ã€å­—æ¯å’Œæ•°å­—")
                    else:
                        st.session_state.current_user = username
                        await kb_manager.user_register(st.session_state.current_user)
                        st.success(f"æ¬¢è¿, {st.session_state.current_user}!")

        st.header("é…ç½®å‚æ•°")
        model_names = list(HIGHSPEED_MODEL_MAPPING.keys())
        selected_model_name = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=model_names,
            index=0
        )
        st.session_state.selected_model = HIGHSPEED_MODEL_MAPPING[selected_model_name]
        st.session_state.chunk_size = st.number_input("æ–‡æœ¬å—å¤§å°", min_value=64, max_value=8192, value=1024, step=64)
        st.session_state.overlap = st.number_input("é‡å å­—ç¬¦æ•°", min_value=0,
                                                   max_value=st.session_state.chunk_size // 2, value=128, step=64)
        st.session_state.special_chars = st.text_input("ç‰¹æ®Šåˆ†éš”ç¬¦ï¼ˆç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼‰", value="")
        st.session_state.special_chars = [char.strip() for char in st.session_state.special_chars.split(
            ",")] if st.session_state.special_chars else None
        st.session_state.top_k = st.number_input("è¿”å›æœ€ç›¸ä¼¼çš„æ®µè½æ•°", min_value=1, max_value=20, value=2, step=1)
        st.session_state.use_rerank = st.checkbox("å¯ç”¨Reranké‡æ’åº", value=False)
        if st.session_state.use_rerank:
            st.info("å°†ä½¿ç”¨Qwen3-rerankeræ¨¡å‹å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº")
    tab1, tab2, tab3 = st.tabs(['çŸ¥è¯†åº“ç®¡ç†', 'çŸ¥è¯†åº“æ„å»ºä¸å±•ç¤º', 'AIçŸ¥è¯†åº“é—®ç­”'])
    with tab1:
        await knowledge_base_management()
    with tab2:
        await RAG_base(rag_system)
    with tab3:
        await rag_with_ai(rag_system)


if 'previous_page' not in st.session_state:
    st.session_state.previous_page = 'RAG'
current_page = 'RAG'
if current_page != st.session_state.previous_page:
    st.session_state.clear()
    st.session_state.previous_page = current_page
asyncio.run(main())
